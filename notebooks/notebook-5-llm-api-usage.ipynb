{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "toc",
    "id": "je6NGL7wiIri"
   },
   "source": [
    "<div style=\"background-color: #002676; padding: 20px;\">\n",
    "<img src=\"https://live-masters-in-computational-social-science.pantheon.berkeley.edu/wp-content/uploads/2025/04/image-3-2.png\" alt=\"MaCSS\" width=\"200\">\n",
    "</div>\n",
    "\n",
    "# **Notebook 5:** Accessing LLMs\n",
    "\n",
    "[wdtmacss@berkeley.edu](mailto:wdtmacss@berkeley.edu)\\\n",
    "**Computational Social Science 1A**\\\n",
    "[Human Psychology and Social Technologies](https://classes.berkeley.edu/content/2025-fall-compss-214a-001-lec-001) \n",
    "Fall 2025\\\n",
    "UC Berkeley [Masters in Computational Social Science](https://macss.berkeley.edu/about/)\n",
    "\n",
    "**Week 5:** Welcome! Accessing frontier models as a data analyst; evaluating open-weight models on huggingface. \n",
    "\n",
    "ðŸ¤–ðŸ¤–ðŸ¤–ðŸ«£\n",
    "\n",
    "---\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "- [Class Summary](#class-summary)\n",
    "- [Introduction: Model Landscape](#model-landscape)\n",
    "- [Today's Research Session](#research-session)\n",
    "- [Today's Lab Session](#lab-session)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_a-AARYW-hC_"
   },
   "source": [
    "<div style=\"padding: 20px;\">\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d6/Hf-logo-with-title.svg\" alt=\"\" width=\"450\">\n",
    "</div>\n",
    "\n",
    "# Class Summary\n",
    "* Today we will ...\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: An Evolving Landscape of LLM Accessibility \n",
    "\n",
    "## Which Model?\n",
    "ChatGPT? Claude? Gemini? DeepSeek? How should we choose which model to use as a data analyst in the current landscape? What are the differing capabilities of different models? What are the different ways to access different models, how much do they cost, how are your queries stored and used? Questions such as these are central to the role of data analysts who wish to make use of the latest advances.  \n",
    "\n",
    "## Local vs. Remote Hosting\n",
    "An important distinction to consider whether you intend to host your own language model or use a model that is provided by another party and hosted elsewhere. Most use cases involve models hosted by a provider these days, and this is certainly the case for propreitary frontier models. But especially for smaller models and open-weight models, instantiating them on your own machine or on cloud computing services is an important possibility and can involve major advantages. The primary drawback is the limitations to the capabilities of smaller and open-weight models -- we will explore some of these limitations systematiicaly later today. \n",
    "\n",
    "### Running Models Locally\n",
    "Smaller models with fewer parameters can be hosted locally on your own computer or on a computing service such as Google Colaboratory, or cloud computing providers. Later in today's class, we will interact with a model hosted on HuggingFace. \n",
    "\n",
    "There are two principal dimensions to consider: first, are the weights of a model openly available; second, how large is the model? As an example, [OpenAI's GPT2 model](https://en.wikipedia.org/wiki/GPT-2) is openly available to use, meaning that the pre-trained model weights are openly accessible. This model includes 1.5 billion paramters. While this is obviously a large number, a model of this size can be read run locally using only CPU (not GPU) resources.   \n",
    "\n",
    "### Interacting with Models Hosted on a Server\n",
    "The primary alternative to running a model locally is to rely on a third part to provide access to the model. For example, Antrhopic provides access to their Calude family of models hosted on their servers. Users can interact with the model in various ways without the difficulties involved in hosting the model themselves, benefitting from the GPU resources and other computing infrastructure that Antrhopic provides to serve the model. The same can be said for other major providers, such as OpenAI and Google. \n",
    "\n",
    "#### Browser-based Chat Interfaces\n",
    "We are likely most familiar with browser-based chat interfaces to large language models. For example, Google's Gemini family of models can be accessed via a conversational chat interface that is browser-based or provided directly as an app for mobile devices.   \n",
    "\n",
    "#### Model APIs\n",
    "As a data analyst, we are likely to require more systematic and programmatic access to model responses than is practical through chat-based interfaces. Moreover, modern chat interfaces include features that might be undesirable for purposes of data analysis, such as memory across conversations, and other advances features that make sense in other contexts. \n",
    "\n",
    "Most major model providers therefore also offer access to their models via an API. Access via API is primarily motivated by the needs of developers, who build apps or services that use the model in some way programmatically. However, as researchers and data analysts we can benefit from model APIs in the same way. \n",
    "\n",
    "In today's session, we will begin to explore the structure of model APIs and how to use them.\n",
    "\n",
    "## Some Popular Developer Platforms\n",
    "OpenAI, Google Anthropic and others provide access to ther models via their developer platforms. \n",
    "\n",
    "### OpenAI\n",
    "Here is an example using from OpenAI's documentation using their Python develoepr tools:\n",
    "\n",
    "```Python \n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Write a short bedtime story about a unicorn.\"\n",
    ")\n",
    "\n",
    "print(response.output_text)\n",
    "```\n",
    "### Anthropic\n",
    "```Python\n",
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What should I search for to find the latest developments in renewable energy?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(message.content)\n",
    "```\n",
    "\n",
    "## Open Source Models\n",
    "One major alternative is the use of Openly accessible models. There are several competetive models that are openly-accessible but would be too large to run on your own machine. For such models, we rely on platforms to host and provide access to those models. \n",
    "\n",
    "One of the primary platforms is [HuggingFace](https://huggingface.co/). We will discuss Huggingface and the [models available there](https://huggingface.co/models) in the later sections of this class. One way to access models on HuggingFace is through [InferenceProviders](https://huggingface.co/docs/inference-providers/en/guides/first-api-call). Here is a basic example:\n",
    "\n",
    "```Python\n",
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"hyperbolic\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Today's Research Session\n",
    "We will dive into the details of several popular models, and the ways that data analysts can access them. Use any resources you like to answer the following set of questions. If you use an AI system to find information, be sure to fact check the AI system! \n",
    "\n",
    "The questions below should be interpreted as focusing on language models (text generation models) primarily, rather than on image models or other Generative AI models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI\n",
    "Using the [documentation for OpenAI's dveeloper platform](https://platform.openai.com/docs/overview), answer the questions below as best you can. \n",
    "\n",
    "* What are some of the models available in OpenAI's developer platform? List at least three, and say how they differ.\n",
    "* How much does it cost to use these three models via the API under the `standard` tier? What is a `token`?\n",
    "* How does OpenAI handle user authentication when people make requests to their API? See the [API documentation](https://platform.openai.com/docs/api-reference/authentication)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic\n",
    "Using the [documentation for Anthropic's dveeloper platform](https://docs.claude.com/en/docs/intro), answer the questions below as best you can. \n",
    "\n",
    "* What are some of the models available? List at least two, and say how they differ.\n",
    "* How much does it cost to use these models via the API? What is MTok?\n",
    "* What is LongContext pricing?\n",
    "* When writing code to interact with API incurs costs, there can be risks involved in accidentally spending too much. Does Anthropic allow you to set hard limits to spending on API calls?\n",
    "* What kinds of accounts are available to use the API from anthropic?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google\n",
    "Using the [documentation for Google AI's dveeloper platform](https://ai.google.dev/), answer the questions below as best you can. \n",
    "\n",
    "* What are some of the models available? List at least two, and say how they differ.\n",
    "* What are the pricing tiers for Gemini API usage?\n",
    "* What are rate limits? \n",
    "* In general, how does Gemini's pricing model differ from Claude and GPT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UC Berkeley Options? \n",
    "Recently, UC Berkeley has begun to provide access to AI models to our campus community. What are the options for interacting with models a researchers, via an API? \n",
    "\n",
    "Check out the follow university pages to explore this question:\n",
    "\n",
    "[UC Berkeley AI Hub](https://ai.berkeley.edu/)\n",
    "[Models Licensed to the UC Berkeley Community](https://ai.berkeley.edu/resources/licensed-generative-ai-tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stretch Goal: Open Source Models on Huggingface\n",
    "This excersise is more open ended, so feel free to return to this later. How are models open-source frontier models such as DeepSeek R1 accessed via huggingface? is there an API you can use? Who provides the model? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Today's Coding Session\n",
    "Today we're going to interact with a handful of smaller, openly-available models hosted on HuggingFace. To facilitate this, I have built a toy system that we can interact with. Our system is based loosely on the structure of OpenAI's [Chat Completion](https://platform.openai.com/docs/guides/text) API. Read through the examples below, and try to answer the questions by using and editing the code provided.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy System: A Huggingface App\n",
    "I have created a [Huggingface Organization for this class](https://huggingface.co/macss-css). Within this organization, I have built a [Huggingface Space for this lab session](https://huggingface.co/spaces/macss-css/llm-lab-two). \n",
    "\n",
    "This Huggingface Space provides an app that allows you to retreive text completions from one of several smaller models. I will introduce the models below, and provide example code for how to send requests to this app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "Here is code to make a request to our system. First we need to import the requests library and specify two things: the URL of our toy system and the name of the model we wish to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Here is the URL of the app\n",
    "URL = \"https://macss-css-llm-lab-two.hf.space\"\n",
    "\n",
    "model = \"gpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below can be used to send a request to our app. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request(request):\n",
    "    response = requests.post(f\"{URL}/predict\", json=request)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of our request object is as follows. First, we need to specify a `message`. To mirror the structure of the OpenAI chat compeltion API, our message object is a list containing a json. The json contains two fields -- a role and a content field. The role is always \"user\" in our setting, and the content is the message string that we would like the model to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_message = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the capital of France?\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One we have this object, we combine it into a json request object that also specifies the model we wish to use (`model`), the number of token that the model is allowed to use to respond (`max_tokens`), and the amount of randomness we wish to see from the model (`temperature` -- this parameters should be between zero and one, where one is the most random and zero is deterministic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_request = {\n",
    "    \"messages\": example_message,\n",
    "    \"model\": model,\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function above to send this request to out system, and see how the model responds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = send_request(example_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the result object to understand it's structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choices': [{'message': {'role': 'assistant', 'content': ' How about England and Scotland, where there are no Englishmen left in Europe.\\nThe point to this article has been made by a lot of people who have had time to read it before reading my analysis: \"Capitalism\\'s Great New Idea\"'}}], 'model': 'gpt2', 'usage': {'prompt_tokens': 6, 'completion_tokens': 41, 'total_tokens': 47}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And print out the text of the model response specifically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " How about England and Scotland, where there are no Englishmen left in Europe.\n",
      "The point to this article has been made by a lot of people who have had time to read it before reading my analysis: \"Capitalism's Great New Idea\"\n"
     ]
    }
   ],
   "source": [
    "model_response = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurable aspects\n",
    "We can configure all aspects of the request object (but for now don't change the model). Here's another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_example_message = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"The letter of the alphabet that comes after 'd' is:\"\n",
    "    }\n",
    "]\n",
    "\n",
    "second_example_request= {\n",
    "    \"messages\": second_example_message,\n",
    "    \"model\": model,\n",
    "    \"temperature\": 0.85,\n",
    "    \"max_tokens\": 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3 + 2 = 5\n",
      "A. In a way, we are asking for more than one part\n"
     ]
    }
   ],
   "source": [
    "new_result = send_request(second_example_request)\n",
    "print(new_result[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excersises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal 1: Days of the Week\n",
    "Does this model understand the days of the week? What day comes after friday? What is the happiest day of the week? Try out different ways of prompting GPT2 to establish this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal 2: List Six Names\n",
    "Can GPT2 count? Ask it to list specific numbers of things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal Three: Judgements about a peice of text\n",
    "Can GPT2 answer questions about a peice of text, such as a social media post? Is this is happy post? Does the post mention a sports team? etc etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes her"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal Four: Your own evaluation task\n",
    "Come up with your own simple evaluation task and test it out on the model. You can be as creative as you like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal Five: testing Different Models\n",
    "Choose one or more of the tasks you examined above. Here are three models you can specify instead of GPT2. Uncomment a line to select a model. Explore how these different models perform on the test you chose. Examine different parameters and models; try find which setup is best at the task or at multiple tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"gpt2\"  \n",
    "# model = \"distilbert/distilgpt2\"  \n",
    "# model = \"EleutherAI/gpt-neo-125M\"  \n",
    "# model = \"facebook/opt-125m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPHKb9d0Xb3zS6cF56CXgtk",
   "provenance": [
    {
     "file_id": "15OtQyqR85bdT2vKDVYmhmJ15-NGhR7VN",
     "timestamp": 1724878025912
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
